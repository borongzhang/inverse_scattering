{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2980328-640d-47e6-afd2-d57030413ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 11:04:03.883453: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "from clu import metric_writers\n",
    "import numpy as np\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "import torch.utils.data as data\n",
    "from tqdm import tqdm\n",
    "\n",
    "import h5py\n",
    "import natsort\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import geometric_transform\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c2de1-35ac-4471-81f7-027c110939ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3443b94d-40b8-40b0-8a77-bb6dfd050389",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70692656-b0fa-4d2f-97e4-b6d703121375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the computational task.\n",
    "\n",
    "L = 4 # number of levels (even number)\n",
    "s = 5 # leaf size\n",
    "r = 3 # rank\n",
    "\n",
    "# Discretization of Omega (n_eta * n_eta).\n",
    "neta = (2**L)*s\n",
    "\n",
    "# Number of sources/detectors (n_sc).\n",
    "# Discretization of the domain of alpha in polar coordinates (n_theta * n_rho).\n",
    "# For simplicity, these values are set equal (n_sc = n_theta = n_rho), facilitating computation.\n",
    "nx = (2**L)*s\n",
    "\n",
    "# Standard deviation for the Gaussian blur.\n",
    "blur_sigma = 0.5\n",
    "\n",
    "# Batch size.\n",
    "batch_size = 16\n",
    "\n",
    "# Number of training datapoints.\n",
    "NTRAIN = 2000\n",
    "\n",
    "# Number of testing datapoints.\n",
    "NTEST = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc5447cb-3303-487a-95eb-7ba6f59ada8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart_polar(coords, neta, nx):\n",
    "    \"\"\"\n",
    "    Transforms coordinates from Cartesian to polar coordinates with custom scaling.\n",
    "\n",
    "    Parameters:\n",
    "    - coords: A tuple or list containing the (i, j) coordinates to be transformed.\n",
    "    - neta: Scaling factor for the radial distance.\n",
    "    - nx: Scaling factor for the angle.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple (rho, theta) representing the transformed coordinates.\n",
    "    \"\"\"\n",
    "    i, j = coords[0], coords[1]\n",
    "    # Calculate the radial distance with a scaling factor.\n",
    "    rho = 2 * jnp.sqrt((i - neta / 2) ** 2 + (j - neta / 2) ** 2) * nx / neta\n",
    "    # Calculate the angle in radians and adjust the scale to fit the specified range.\n",
    "    theta = ((jnp.arctan2((neta / 2 - j), (i - neta / 2))) % (2 * jnp.pi)) * nx / jnp.pi / 2\n",
    "    return theta, rho + neta // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe182499-eba6-4333-b448-7ce6dc6d8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to precompute the transformation matrix\n",
    "def precompute_transform_matrix(neta, nx, cart_polar):\n",
    "    cart_mat = jnp.zeros((neta**2, nx, nx))\n",
    "\n",
    "    for i in range(nx):\n",
    "        for j in range(nx):\n",
    "            # Create a dummy matrix with a single one at position (i, j) and zeros elsewhere.\n",
    "            mat_dummy = jnp.zeros((nx, nx))\n",
    "            mat_dummy = lax.dynamic_update_index_in_dim(mat_dummy, (i, j), 1)\n",
    "            # Pad the dummy matrix in polar coordinates to cover the target space in Cartesian coordinates.\n",
    "            pad_dummy = jnp.pad(mat_dummy, ((0, 0), (neta // 2, neta // 2)), 'edge')\n",
    "            # Apply the geometric transformation to map the dummy matrix to polar coordinates\n",
    "            cart_mat = lax.dynamic_update_index_in_dim(cart_mat, (slice(None), i, j), \n",
    "                                                        jnp.ravel(geometric_transform(pad_dummy, cart_polar, output_shape=[neta, neta], mode='grid-wrap')))\n",
    "\n",
    "    cart_mat = jnp.reshape(cart_mat, (neta**2, nx**2))\n",
    "    # Removing small values\n",
    "    cart_mat = jnp.where(jnp.abs(cart_mat) > 0.001, cart_mat, 0)\n",
    "    return cart_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c961d-da9c-443b-aec0-8747b1f872fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'shepp_logan'\n",
    "\n",
    "# Define a function to load and preprocess perturbation data (eta)\n",
    "def load_and_preprocess_eta(name, NTRAIN, neta, blur_sigma):\n",
    "    with h5py.File(f'{name}/eta.h5', 'r') as f:\n",
    "        # Read eta data, apply Gaussian blur, and reshape\n",
    "        eta_re = f[list(f.keys())[0]][:NTRAIN, :].reshape(-1, neta, neta)\n",
    "        blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "        eta_re = jnp.stack([blur_fn(eta_re[i, :, :]) for i in range(NTRAIN)]).astype('float32')\n",
    "    return eta_re\n",
    "\n",
    "# Define a function to load and preprocess scatter data (Lambda)\n",
    "def load_and_preprocess_scatter(name, NTRAIN, nx):\n",
    "    with h5py.File(f'{name}/scatter.h5', 'r') as f:\n",
    "        keys = natsort.natsorted(f.keys())\n",
    "        \n",
    "        # Process real part of scatter data\n",
    "        tmp1 = f[keys[3]][:NTRAIN, :].reshape((-1, nx, nx))\n",
    "        tmp2 = f[keys[4]][:NTRAIN, :].reshape((-1, nx, nx))\n",
    "        tmp3 = f[keys[5]][:NTRAIN, :].reshape((-1, nx, nx))\n",
    "        scatter_re = jnp.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "        \n",
    "        # Process imaginary part of scatter data\n",
    "        tmp1 = f[keys[0]][:NTRAIN, :].reshape((-1, nx, nx))\n",
    "        tmp2 = f[keys[1]][:NTRAIN, :].reshape((-1, nx, nx))\n",
    "        tmp3 = f[keys[2]][:NTRAIN, :].reshape((-1, nx, nx))\n",
    "        scatter_im = jnp.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "        \n",
    "        # Combine real and imaginary parts\n",
    "        scatter = jnp.stack((scatter_re, scatter_im), axis=1).astype('float32')\n",
    "    return scatter\n",
    "\n",
    "# Load and preprocess perturbation data\n",
    "eta_re = load_and_preprocess_eta(name, NTRAIN, neta, blur_sigma)\n",
    "\n",
    "# Load and preprocess scatter data\n",
    "scatter = load_and_preprocess_scatter(name, NTRAIN, nx)\n",
    "\n",
    "# Clean up temporary variables to free memory\n",
    "del scatter_re, scatter_im, tmp1, tmp2, tmp3\n",
    "\n",
    "# Convert JAX arrays to NumPy arrays as TensorFlow works with NumPy arrays\n",
    "eta_re_np = jax.device_get(eta_re)\n",
    "scatter_np = jax.device_get(scatter)\n",
    "\n",
    "# Create a TensorFlow dataset for training\n",
    "trn_dataset = tf.data.Dataset.from_tensor_slices((scatter_np, eta_re_np))\n",
    "trn_dataset = trn_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "trn_dataset = trn_dataset.shuffle(buffer_size=200)\n",
    "trn_dataset = trn_dataset.batch(BATCH_SIZE)\n",
    "dataset = eval_dataloader = trn_dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e72b6-2b15-422a-88ae-5f5a4f223f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the F^* layer using Flax\n",
    "class Fstar(nn.Module):\n",
    "    nx: int\n",
    "    neta: int\n",
    "    cart_mat: jnp.ndarray\n",
    "\n",
    "    def setup(self):\n",
    "        self.pre1 = self.param('pre1', lambda key, shape: jax.random.uniform(key, shape))\n",
    "        self.post1 = self.param('post1', lambda key, shape: jax.random.uniform(key, shape))\n",
    "        self.cos_kernel1 = self.param('cos_kernel1', lambda key, shape: jax.random.uniform(key, shape))\n",
    "        self.sin_kernel1 = self.param('sin_kernel1', lambda key, shape: jax.random.uniform(key, shape))\n",
    "        self.cos_kernel2 = self.param('cos_kernel2', lambda key, shape: jax.random.uniform(key, shape))\n",
    "        self.sin_kernel2 = self.param('sin_kernel2', lambda key, shape: jax.random.uniform(key, shape))\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # Separate real and imaginary parts of inputs\n",
    "        R, I = inputs[:, 0, :, :], inputs[:, 1, :, :]\n",
    "        \n",
    "        # Define rotation function\n",
    "        def rotationindex(n):\n",
    "            index = jnp.reshape(jnp.arange(0, n**2, 1), [n, n])\n",
    "            return jnp.concatenate([jnp.roll(index, shift=[-i,-i], axis=[0,1]) for i in range(n)], 0)\n",
    "        \n",
    "        rindex = lambda d: jax.lax.dynamic_slice(jnp.reshape(d, [-1]), rotationindex(self.nx), [self.nx * self.nx])\n",
    "        \n",
    "        Rs = jax.vmap(rindex)(R)\n",
    "        Rs = jnp.reshape(Rs, [-1, self.nx, self.nx])\n",
    "        Is = jax.vmap(rindex)(I)\n",
    "        Is = jnp.reshape(Is, [-1, self.nx, self.nx])\n",
    "        \n",
    "        def helper(pre, post, kernel2, kernel1, data):\n",
    "            return jnp.matmul(post, jnp.multiply(kernel2, jnp.matmul(jnp.multiply(data, pre), kernel1)))  \n",
    "        \n",
    "        output_polar = helper(self.pre1, self.post1, self.cos_kernel1, self.cos_kernel2, Rs) \\\n",
    "                      + helper(self.pre2, self.post2, self.sin_kernel1, self.sin_kernel2, Rs) \\\n",
    "                      + helper(self.pre3, self.post3, self.cos_kernel2, self.sin_kernel1, Is) \\\n",
    "                      + helper(self.pre4, self.post4, self.sin_kernel2, self.cos_kernel1, Is)\n",
    "        \n",
    "        output_polar = jnp.reshape(output_polar, (-1, self.nx, self.nx))\n",
    "        \n",
    "        # Convert from polar to Cartesian coordinates\n",
    "        def polar_to_cart(x):\n",
    "            x = jnp.reshape(x, (self.nx**2, 1))\n",
    "            x = jnp.dot(self.cart_mat, x)\n",
    "            return jnp.reshape(x, (self.neta, self.neta))\n",
    "        \n",
    "        output_cart = jax.vmap(polar_to_cart)(output_polar)\n",
    "        return jnp.reshape(output_cart, (-1, self.neta, self.neta, 1))\n",
    "\n",
    "# Define the main model using Flax\n",
    "class MyModel(nn.Module):\n",
    "    nx: int\n",
    "    neta: int\n",
    "    cart_mat: jnp.ndarray\n",
    "    num_cnn: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.fstar_layer = Fstar(nx=self.nx, neta=self.neta, cart_mat=self.cart_mat)\n",
    "        self.convs = [nn.Conv(features=6, kernel_size=(3, 3), padding='SAME') for _ in range(self.num_cnn - 1)]\n",
    "        self.final_conv = nn.Conv(features=1, kernel_size=(3, 3), padding='SAME')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        y1 = self.fstar_layer(inputs[:, :, :, :, 0])\n",
    "        y2 = self.fstar_layer(inputs[:, :, :, :, 1])\n",
    "        y3 = self.fstar_layer(inputs[:, :, :, :, 2])\n",
    "\n",
    "        y = jnp.concatenate([y1, y2, y3], axis=-1)\n",
    "\n",
    "        for conv_layer in self.convs:\n",
    "            y = conv_layer(y)\n",
    "            y = jax.nn.relu(y)\n",
    "        \n",
    "        y = self.final_conv(y)\n",
    "        \n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58a7ff-8882-4061-b06f-88565528991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = MyModel(nx, neta, cart_mat, NUM_CNN)\n",
    "\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rng, inp_rng, init_rng = jax.random.split(rng, 3)\n",
    "inp = jax.random.normal(inp_rng, (batch_size, L1, L1, 2))  \n",
    "# Define an optimizer\n",
    "optimizer = optax.adam(learning_rate=1e-4)\n",
    "params = model.init(init_rng, inp)  # Initialize parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e2b130-3b2c-459e-b7e8-f30bd728d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state\n",
    "\n",
    "model_state = train_state.TrainState.create(apply_fn=model.apply,\n",
    "                                            params=params,\n",
    "                                            tx=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d2a2fc-6a8b-4c19-81d7-f4b13f18cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_acc(state, params, batch):\n",
    "    x, y = batch\n",
    "    # Obtain the logits and predictions of the model for the input data\n",
    "    pred = state.apply_fn(params, x)\n",
    "       \n",
    "    # Calculate the loss and accuracy\n",
    "    loss = jnp.mean((pred - y) ** 2)\n",
    "    acc = jnp.sqrt(loss/jnp.mean(y ** 2))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a7bea3-f75e-4dcd-bac8-c31563619a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_loader))\n",
    "calculate_loss_acc(model_state, model_state.params, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6369765d-6c95-44d9-804a-b4bc76a81aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit  # Jit the function for efficiency\n",
    "def train_step(state, batch):\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(calculate_loss_acc,  # Function to calculate the loss\n",
    "                                 argnums=1,  # Parameters are second argument of the function\n",
    "                                 has_aux=True  # Function has additional outputs, here accuracy\n",
    "                                )\n",
    "    # Determine gradients for current model, parameters and batch\n",
    "    (loss, acc), grads = grad_fn(state, state.params, batch)\n",
    "    # Perform parameter update with gradients and optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Return state and any other value we might want\n",
    "    return state, loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c3d478-6432-456d-b16b-c38bc70822df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit  # Jit the function for efficiency\n",
    "def eval_step(state, batch):\n",
    "    # Determine the accuracy\n",
    "    _, acc = calculate_loss_acc(state, state.params, batch)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "785b7e38-4ba3-44fa-951e-b304370da38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(state, data_loader, num_epochs=100):\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            state, loss, acc = train_step(state, batch)\n",
    "        print(acc)\n",
    "            \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "453354c6-5733-4793-a67c-8a98704cb229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62765276\n",
      "0.5699285\n",
      "0.51149094\n",
      "0.4364011\n",
      "0.43604264\n",
      "0.3773166\n",
      "0.36706853\n",
      "0.38370332\n",
      "0.35950708\n",
      "0.3578673\n",
      "0.33781496\n",
      "0.33543062\n",
      "0.3399269\n",
      "0.30684948\n",
      "0.30500194\n",
      "0.2904274\n",
      "0.28377196\n",
      "0.30791003\n",
      "0.30995208\n",
      "0.30437276\n",
      "0.3007315\n",
      "0.27253976\n",
      "0.286515\n",
      "0.26192582\n",
      "0.25777122\n",
      "0.25902832\n",
      "0.25168607\n",
      "0.2622497\n",
      "0.2806948\n",
      "0.26001588\n",
      "0.2630823\n",
      "0.24773355\n",
      "0.23084229\n",
      "0.25927553\n",
      "0.26018977\n",
      "0.2271517\n",
      "0.24457313\n",
      "0.22907971\n",
      "0.23938344\n",
      "0.24449262\n",
      "0.24650888\n",
      "0.26309568\n",
      "0.22288127\n",
      "0.23615713\n",
      "0.220491\n",
      "0.21007022\n",
      "0.21267845\n",
      "0.23347838\n",
      "0.22431682\n",
      "0.20537148\n",
      "0.20106609\n",
      "0.22448424\n",
      "0.20818888\n",
      "0.21766713\n",
      "0.21276915\n",
      "0.18663907\n",
      "0.19759814\n",
      "0.18237337\n",
      "0.19505891\n",
      "0.22396858\n",
      "0.19007027\n",
      "0.20751317\n",
      "0.21308897\n",
      "0.19851148\n",
      "0.19169652\n",
      "0.1964183\n",
      "0.19370717\n",
      "0.18364118\n",
      "0.18232246\n",
      "0.18579541\n",
      "0.18737678\n",
      "0.18589503\n",
      "0.19248821\n",
      "0.18174112\n",
      "0.17948163\n",
      "0.1833093\n",
      "0.1747682\n",
      "0.18085857\n",
      "0.18993571\n",
      "0.18191735\n",
      "0.1803571\n",
      "0.17224161\n",
      "0.17769665\n",
      "0.17789787\n",
      "0.1709675\n",
      "0.1734087\n",
      "0.16577762\n",
      "0.17144592\n",
      "0.17202622\n",
      "0.1773083\n",
      "0.16912267\n",
      "0.18149474\n",
      "0.19026102\n",
      "0.16831759\n",
      "0.16443536\n",
      "0.17238052\n",
      "0.1685627\n",
      "0.15964733\n",
      "0.15995418\n",
      "0.16987945\n",
      "0.16009602\n",
      "0.16311416\n",
      "0.1623361\n",
      "0.16870178\n",
      "0.15796262\n",
      "0.16113217\n",
      "0.17512524\n",
      "0.1632514\n",
      "0.15669371\n",
      "0.16962245\n",
      "0.16304196\n",
      "0.17570935\n",
      "0.15682723\n",
      "0.15869091\n",
      "0.15771756\n",
      "0.15615165\n",
      "0.16473159\n",
      "0.15718493\n",
      "0.15184718\n",
      "0.16273645\n",
      "0.15457919\n",
      "0.15645492\n",
      "0.15810803\n",
      "0.14634258\n",
      "0.14932172\n",
      "0.1507779\n",
      "0.15673573\n",
      "0.15670046\n",
      "0.15754442\n",
      "0.15189596\n",
      "0.14963093\n",
      "0.15076798\n",
      "0.15086411\n",
      "0.14533438\n",
      "0.14786994\n",
      "0.14553785\n",
      "0.15424548\n",
      "0.1471799\n",
      "0.15221839\n",
      "0.14972831\n",
      "0.15011002\n",
      "0.15058339\n",
      "0.1479321\n",
      "0.14796986\n",
      "0.14244023\n",
      "0.1567557\n",
      "0.15384574\n",
      "0.15503798\n",
      "0.15008016\n",
      "0.15198597\n",
      "0.14262478\n",
      "0.13809972\n",
      "0.14264038\n",
      "0.14955355\n",
      "0.1518635\n",
      "0.14912395\n",
      "0.14558555\n",
      "0.14038092\n",
      "0.13911127\n",
      "0.14320086\n",
      "0.14740703\n",
      "0.14204733\n",
      "0.1415545\n",
      "0.13973264\n",
      "0.13985553\n",
      "0.13785028\n",
      "0.1401543\n",
      "0.13919893\n",
      "0.13422115\n",
      "0.14046882\n",
      "0.14111716\n",
      "0.136359\n",
      "0.13411395\n",
      "0.13584511\n",
      "0.13812728\n",
      "0.13897048\n",
      "0.14425896\n",
      "0.13677879\n",
      "0.13718988\n",
      "0.12953362\n",
      "0.1329801\n",
      "0.1414406\n",
      "0.14284055\n",
      "0.13577396\n",
      "0.13064909\n",
      "0.13306297\n",
      "0.13557735\n",
      "0.1262879\n",
      "0.13372122\n",
      "0.13632149\n",
      "0.13683416\n",
      "0.13407923\n",
      "0.13012353\n",
      "0.13425682\n",
      "0.13259916\n",
      "0.13781874\n",
      "0.12690006\n",
      "0.1292552\n",
      "0.1312685\n",
      "0.1316681\n",
      "0.14150319\n",
      "0.12776135\n",
      "0.12940596\n",
      "0.13161555\n",
      "0.13169152\n",
      "0.13376978\n",
      "0.13256912\n",
      "0.13506949\n",
      "0.13235892\n",
      "0.134843\n",
      "0.12987953\n",
      "0.123748645\n",
      "0.1355911\n",
      "0.13082798\n",
      "0.13373215\n",
      "0.13059\n",
      "0.1335101\n",
      "0.12896879\n",
      "0.12674817\n",
      "0.12867902\n",
      "0.1374138\n",
      "0.12031628\n",
      "0.12561567\n",
      "0.12252492\n",
      "0.12571865\n",
      "0.12480029\n",
      "0.122763574\n",
      "0.121155396\n",
      "0.12085426\n",
      "0.12566094\n",
      "0.13236494\n",
      "0.12561908\n",
      "0.121289\n",
      "0.12992094\n",
      "0.117367946\n",
      "0.12182819\n",
      "0.12355454\n",
      "0.123694226\n",
      "0.12511185\n",
      "0.122215495\n",
      "0.12419734\n",
      "0.12028347\n",
      "0.12325033\n",
      "0.12177549\n",
      "0.12725073\n",
      "0.1259661\n",
      "0.12484698\n",
      "0.12369662\n",
      "0.12065595\n",
      "0.11988834\n",
      "0.11971947\n",
      "0.11703633\n",
      "0.12039459\n",
      "0.12100526\n",
      "0.12087415\n",
      "0.12777403\n",
      "0.113248885\n",
      "0.12565352\n",
      "0.12653624\n",
      "0.12046443\n",
      "0.124556236\n",
      "0.12411262\n",
      "0.115197435\n",
      "0.117975004\n",
      "0.115254164\n",
      "0.11813784\n",
      "0.11989353\n",
      "0.11588009\n",
      "0.124668024\n",
      "0.12237539\n",
      "0.118368775\n",
      "0.11611481\n",
      "0.12112163\n",
      "0.12375912\n",
      "0.12184331\n",
      "0.124980494\n",
      "0.12001375\n",
      "0.11960879\n",
      "0.11884918\n",
      "0.1268618\n",
      "0.11364437\n",
      "0.11591083\n",
      "0.11713543\n",
      "0.11476257\n",
      "0.1200588\n",
      "0.11531665\n",
      "0.11302457\n",
      "0.11503804\n",
      "0.11726803\n",
      "0.11039862\n",
      "0.11353736\n",
      "0.12153591\n",
      "0.11579185\n",
      "0.11652565\n",
      "0.11986186\n",
      "0.11733205\n",
      "0.11682658\n",
      "0.11905493\n",
      "0.12159357\n",
      "0.11790405\n",
      "0.117849596\n",
      "0.12257884\n",
      "0.11621544\n",
      "0.11451284\n",
      "0.11651891\n",
      "0.11001924\n",
      "0.114844136\n",
      "0.11706725\n",
      "0.11887202\n",
      "0.11590325\n",
      "0.11658447\n",
      "0.10717339\n",
      "0.11241627\n",
      "0.12089035\n",
      "0.112974666\n",
      "0.11670561\n",
      "0.111526765\n",
      "0.11353361\n",
      "0.11611515\n",
      "0.114196144\n",
      "0.11153223\n",
      "0.112235345\n",
      "0.113524556\n",
      "0.110073514\n",
      "0.109841004\n",
      "0.11515549\n",
      "0.10934714\n",
      "0.10900684\n",
      "0.109896705\n",
      "0.1064624\n",
      "0.10927249\n",
      "0.11310264\n",
      "0.10939397\n",
      "0.1079905\n",
      "0.11242276\n",
      "0.11116255\n",
      "0.10643049\n",
      "0.11286624\n",
      "0.11587573\n",
      "0.110626414\n",
      "0.112534165\n",
      "0.111736566\n",
      "0.10884606\n",
      "0.1114302\n",
      "0.10535716\n",
      "0.11060202\n",
      "0.108967155\n",
      "0.10614903\n",
      "0.10501239\n",
      "0.10786877\n",
      "0.112658724\n",
      "0.10903492\n",
      "0.11391949\n",
      "0.10484736\n",
      "0.1112889\n",
      "0.106055535\n",
      "0.10212881\n",
      "0.10895861\n",
      "0.11222678\n",
      "0.10815472\n",
      "0.10731513\n",
      "0.10605595\n",
      "0.101936825\n",
      "0.109740146\n",
      "0.10490491\n",
      "0.10516013\n",
      "0.10616402\n",
      "0.10689855\n",
      "0.1041833\n",
      "0.108312376\n",
      "0.10722581\n",
      "0.10958765\n",
      "0.10874123\n",
      "0.11094569\n",
      "0.11076138\n",
      "0.102821335\n",
      "0.10785496\n",
      "0.10809107\n",
      "0.10520912\n",
      "0.1028026\n",
      "0.10335428\n",
      "0.10468149\n",
      "0.104155466\n",
      "0.10929952\n",
      "0.102983505\n",
      "0.10549261\n",
      "0.104871854\n",
      "0.110377364\n",
      "0.107053384\n",
      "0.1070419\n",
      "0.10201109\n",
      "0.10166901\n",
      "0.10910741\n",
      "0.105561994\n",
      "0.105189905\n",
      "0.10746308\n",
      "0.10533858\n",
      "0.1020408\n",
      "0.10471549\n",
      "0.10388806\n",
      "0.10430977\n",
      "0.1078674\n",
      "0.10369247\n",
      "0.10394647\n",
      "0.09877312\n",
      "0.10491513\n",
      "0.100229904\n",
      "0.09999615\n",
      "0.10355373\n",
      "0.10448227\n",
      "0.099928506\n",
      "0.10597912\n",
      "0.10278721\n",
      "0.10023757\n",
      "0.10136867\n",
      "0.09674503\n",
      "0.10837699\n",
      "0.102796696\n",
      "0.1056483\n",
      "0.100337826\n",
      "0.09835594\n",
      "0.101735815\n",
      "0.09839947\n",
      "0.09545651\n",
      "0.09800288\n",
      "0.10630323\n",
      "0.09899521\n",
      "0.10001449\n",
      "0.10104089\n",
      "0.101944566\n",
      "0.09867388\n",
      "0.096642695\n",
      "0.098741904\n",
      "0.10440913\n",
      "0.10120287\n",
      "0.10509146\n",
      "0.10356454\n",
      "0.10186899\n",
      "0.09966064\n",
      "0.10274451\n",
      "0.097658694\n",
      "0.102904595\n",
      "0.096914485\n",
      "0.0969788\n",
      "0.09909756\n",
      "0.09932884\n",
      "0.10047785\n",
      "0.09348008\n",
      "0.101016775\n",
      "0.10049725\n",
      "0.101302095\n",
      "0.09752847\n",
      "0.09929576\n",
      "0.095974326\n",
      "0.09813538\n",
      "0.10415158\n",
      "0.09787876\n",
      "0.09964666\n",
      "0.096960306\n",
      "0.10241381\n",
      "0.09575912\n",
      "0.092771724\n",
      "0.099582076\n",
      "0.096066944\n",
      "0.09799671\n",
      "0.099501975\n",
      "0.094077356\n",
      "0.10152804\n",
      "0.09792346\n",
      "0.09629103\n",
      "0.09710806\n",
      "0.101344615\n",
      "0.09557131\n",
      "0.099434674\n",
      "0.09908022\n",
      "0.09859705\n",
      "0.09533041\n",
      "0.09441035\n",
      "0.09700184\n",
      "0.0950679\n",
      "0.10030664\n",
      "0.09811351\n",
      "0.097138785\n",
      "0.098733306\n",
      "0.09682907\n",
      "0.09885794\n",
      "0.09830615\n",
      "0.09346555\n",
      "0.100841306\n",
      "0.09247167\n",
      "0.10118163\n",
      "0.09822305\n",
      "0.091169536\n",
      "0.09328846\n",
      "0.10251593\n",
      "0.09934017\n",
      "0.09802131\n",
      "0.09696384\n",
      "0.101415336\n",
      "0.09192456\n"
     ]
    }
   ],
   "source": [
    "trained_model_state = train_model(model_state, data_loader, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c5c7cd-3de7-479c-bba1-baa98c044d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaafbb6-5903-4a8b-a4e4-55cf2f0ea077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxflax",
   "language": "python",
   "name": "jaxflax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
